{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pylab\n",
    "import os\n",
    "from keras.layers import Layer, InputSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.layers import Input, LSTM, Dropout, Conv1D\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers.core import Dense, Lambda, Activation, Flatten\n",
    "from keras import objectives\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping \n",
    "from statistics import mean, median\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from google.cloud import bigquery\n",
    "K.set_session\n",
    "plt.style.use('ggplot')\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='/Users/tatsuyahagiwara/d/bigquery.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "前処理\n",
    "get_data: データを前処理して深層学習で扱える形に変換する\n",
    "transform_data: データを正規化してLSTMで学習が容易な形に変更する。\n",
    "\"\"\"\n",
    "\n",
    "def get_data(data, time_steps):\n",
    "    docX, docY = [], []\n",
    "    for i in range(len(data)-time_steps):\n",
    "        docX.append(data[i:i+time_steps])\n",
    "        docY.append(data[i+time_steps])\n",
    "    alsX = np.array(docX)\n",
    "    alsY = np.array(docY)\n",
    "    return alsX, alsY\n",
    "\n",
    "def transform_data(data, inverse_option, scaler):\n",
    "    data_shape = data.shape\n",
    "    if inverse_option is True:\n",
    "        data = scaler.inverse_transform(data)\n",
    "    else:\n",
    "        data = scaler.fit_transform(data)\n",
    "    data = data.reshape(data_shape)\n",
    "    return data, scaler\n",
    "\n",
    "def prepare_data(original_data, time_steps):\n",
    "    copy_data = original_data.copy()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "    data, scaler = transform_data(data=copy_data, \n",
    "                              inverse_option=False, scaler=scaler)\n",
    "    \n",
    "    x,y = get_data(data, time_steps=time_steps)\n",
    "    return x, y, scaler  \n",
    "\n",
    "\n",
    "\n",
    "# データのplot\n",
    "def show_graph(day, data, label, color=\"b\"):\n",
    "    pylab.figure(figsize=(14, 8))\n",
    "    pylab.subplot(211)\n",
    "    pylab.xlabel('time')\n",
    "    pylab.ylabel('temperature')\n",
    "    pylab.plot(day, data, color=color, label=label)\n",
    "    pylab.legend(loc='upper right')\n",
    "    pylab.show()\n",
    "\n",
    "# 学習データとその予測データのplot\n",
    "def predict_model_show_graph(day, x, y_, scaler, model, time_steps):\n",
    "    preds = model.predict(x)\n",
    "    x = np.reshape(y_, (len(y_),))\n",
    "    x_scale, scaler = transform_data(data=y_, inverse_option=True, scaler=scaler)\n",
    "    predict_scale, scaler = transform_data(data=preds, inverse_option=True, scaler=scaler)\n",
    "    mse_value = [(v - p_v)**2 for v, p_v in zip(x_scale[:,0], predict_scale[:,0])]\n",
    "    return predict_scale, x_scale, mse_value\n",
    "\n",
    "# 閾値計算\n",
    "def calculate_mse(value, predict_value, variance=1.0):\n",
    "    value = value[:, 0]\n",
    "    predict_value = predict_value[:, 0]\n",
    "    mse_value = [(v - p_v)**2 for v, p_v in zip(value, predict_value)]\n",
    "    return np.array(mse_value)\n",
    "\n",
    "# 閾値とデータplot\n",
    "def show_graph_threshold(day, data, threshold, label, color=\"b\", fix_threshold=True):\n",
    "    pylab.figure(figsize=(14, 8))\n",
    "    pylab.subplot(211)\n",
    "    pylab.xlabel('time')\n",
    "    pylab.ylabel('anomaly score')\n",
    "    if fix_threshold is True:\n",
    "        pylab.plot(day, np.repeat(threshold, day.shape[0]), 'k-', color = \"k\", ls = \"dashed\")\n",
    "    else:\n",
    "        pylab.plot(day, threshold, 'k-', color = \"k\", ls = \"dashed\")\n",
    "    pylab.plot(day, data, \"r\", label='Anomaly Score test')\n",
    "    pylab.legend(loc='upper right')\n",
    "    pylab.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "モデルの作成と重みのロード\n",
    "\"\"\"\n",
    "time_steps = 12\n",
    "window = time_steps\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(time_steps, 1),\n",
    "         stateful=False,return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.load_weights('/Users/tatsuyahagiwara/d/cnt_label/week_dir/model.ep200.h5')\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQueryからSQLでデータ取ってくる\n",
    "client = bigquery.Client(project='neon-opus-132123')\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.use_legacy_sql = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一日分データ（2018/07/01~2018/07/30）30d\n",
    "# 一週間分データ(2018/01/08~2018/02/26) 7w\n",
    "# 一ヶ月分データ(2018/01/01~2018/07/01) 7m\n",
    "query =  \"\"\"\n",
    "#standardSQL\n",
    "select format_timestamp(\"%Y%m%d%H\", time) AS hours,\n",
    "original_keyword,\n",
    "COUNT(original_keywords) AS cnt\n",
    "from `search_logs.search_log_*`, UNNEST(original_keywords) as original_keyword\n",
    "where original_keyword is not NULL\n",
    "AND _TABLE_SUFFIX BETWEEN '20180301' AND '20180301'\n",
    "GROUP BY hours, original_keyword\n",
    "ORDER BY hours asc\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query( query, job_config=job_config)\n",
    "results = query_job.result()\n",
    "\n",
    "# hour, words, cntのDataFrameの作成\n",
    "\n",
    "csv=[[i.hours, i.original_keyword, i.cnt] for i in list(results)]\n",
    "csv=pd.DataFrame(csv)\n",
    "listed=['hours','words','cnt']\n",
    "csv.columns = listed\n",
    "csv['hours']=pd.to_datetime(csv['hours'], format='%Y%m%d%H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34715"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=set(csv['words'])\n",
    "words=list(words)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワードごとの時系列データ作成\n",
    "\n",
    "slist = []\n",
    "for word in words:\n",
    "    group = csv[csv['words'] == word]\n",
    "    if len(group)>=24:\n",
    "        slist.append(group)\n",
    "        \n",
    "cnt_dic = {}\n",
    "for v in slist:\n",
    "    for hour, word,cnt in zip(v['hours'],v['words'],v['cnt']):\n",
    "        cnt_dic.setdefault(word, []).append(cnt)\n",
    "        \n",
    "words_list=list(cnt_dic.keys())  \n",
    "words=[i for i in words if i in words_list]\n",
    "\n",
    "\n",
    "hour_dic = {}\n",
    "for v in slist:\n",
    "    for hour, word,cnt in zip(v['hours'],v['words'],v['cnt']):\n",
    "        hour_dic.setdefault(word, []).append(hour)\n",
    "        \n",
    "        \n",
    "cnt_df = pd.DataFrame.from_dict(cnt_dic, orient='index')\n",
    "cnt_df=cnt_df.T  #applymap(lambda x: x*1000)\n",
    "\n",
    "\n",
    "hour_df = pd.DataFrame.from_dict(hour_dic, orient='index')\n",
    "hour_df=hour_df.T \n",
    "\n",
    "hour_listed=[]\n",
    "cnt_listed=[]\n",
    "for i in words:\n",
    "    hour_listed.append([hour_df[i],i])\n",
    "    cnt_listed.append([cnt_df[i],i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 抽出ワード数\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検知したバーストワードをlistに保存\n",
    "\n",
    "csvs=[]\n",
    "original_threshold = 270.2200 \n",
    "train_data_mean=4.964\n",
    "num_words=len(words)\n",
    "for i in range(num_words):\n",
    "    cnt=pd.DataFrame(list(cnt_listed[i][0])).dropna()\n",
    "    hour=pd.DataFrame(list(hour_listed[i][0])).dropna()\n",
    "    hours=np.array(hour[window:])\n",
    "    wo=cnt_listed[i][1] # each word\n",
    "    \n",
    "    x_predict, y_ ,scaler = prepare_data(cnt, time_steps)\n",
    "    predicted, x_scale, mse_value = predict_model_show_graph(hour[window:], x_predict, y_, scaler, model, time_steps)\n",
    "    mse_value = calculate_mse(x_scale, predicted)\n",
    "    \n",
    "    N=int(np.mean(x_scale)/2)\n",
    "    high_threshold = original_threshold*N  # 閾値\n",
    "    if high_threshold<original_threshold:\n",
    "        #print('high_threshold: {}'.format(original_threshold))\n",
    "        #show_graph_threshold(hour[window:], mse_value, original_threshold, 'Anomaly Score test', \"r\")\n",
    "        # 急上昇ワードの時間を表示\n",
    "        for mse, hour in zip(mse_value, hours):\n",
    "            if mse >=original_threshold:\n",
    "                csvs.append([hour, wo])\n",
    "    else:\n",
    "        #print('high_threshold: {}'.format(high_threshold))\n",
    "        #show_graph_threshold(hour[window:], mse_value, high_threshold, 'Anomaly Score test', \"r\")\n",
    "        # 急上昇ワードの時間を表示\n",
    "        for mse, hour in zip(mse_value, hours):\n",
    "            if mse >=high_threshold:\n",
    "                csvs.append([hour, wo])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(csvs, columns=['hour','words'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nに日にちを指定\n",
    "No='N'\n",
    "\n",
    "# 対象ではない日のデータを削除\n",
    "dff=[]\n",
    "for i,v in df.iterrows():\n",
    "    if str(v['hour'])[10:12] not in No:\n",
    "        dff.append([v['hour'],v['words']])\n",
    "        \n",
    "dff=pd.DataFrame(df, columns=['hour','words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字列に変換し、重複ワード削除\n",
    "dff=dff.applymap(lambda x: str(x))\n",
    "dff=dff.drop_duplicates(subset='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2018-03-01T11:00:00.000000000]</td>\n",
       "      <td>レディース</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              hour  words\n",
       "0  [2018-03-01T11:00:00.000000000]  レディース"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff #バーストワードと日時"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
